# SuperScaler

- ## What is SuperScaler
  SuperScaler is a framework that helps you to distribute your single device deep learning model into multi-device and multi-machines for efficient paralell-training.

- ## Introduction
  Most state-of-the-art deep learning models leverage parallel training on multiple devices/servers to process complicated computation on large volume of data. Scaling a deep learning model for parallel training is challenging as it combining the complexities of different parallelization strategies, consistencies, communication strategies, underlying data links and various optimizations methods. 
  
  SuperScaler aims to provide a comprehensive solution to scaling deep learning training. SuperScaler introduces a unified flexible framework that could integrate high-performance training techniques including auto-parallelization and communication compression etc. With SuperScaler, users could easily scale their deep learning models with only little changes on the code.

- ## Microsoft Open Source Code of Conduct

  This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).

  Resources:
  - [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/)
  - [Microsoft Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)
  - Contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with questions or concerns
