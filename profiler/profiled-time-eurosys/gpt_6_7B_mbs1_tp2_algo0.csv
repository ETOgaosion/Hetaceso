op_name,forward-compute,backward-compute,input_size,output_size,weights,activations,fwd_reserved,bwd_reserved
encoder-embedding,890.156,1822.786,0.008,16.000,216.000,24.018,39.982,264.000
enc-1st-layernorm,88.245,264.113,16.000,32.000,0.000,16.016,0.000,128.000
enc-attention-qkv,1302.223,2962.031,32.000,40.000,48.000,24.000,40.000,222.000
enc-attention-score,375.658,675.626,40.000,152.000,0.000,128.000,128.000,296.000
enc-attention-softmax,308.720,404.807,152.000,152.000,0.000,128.000,0.000,384.000
enc-attention-dropout,482.861,415.908,152.000,152.000,0.000,192.000,0.000,512.000
enc-attention-context,377.295,646.425,152.000,24.000,0.000,8.000,12.000,144.000
enc-attention-dense,1042.444,785.171,24.000,32.008,16.000,16.000,0.000,64.000
enc-post-attention-dropout,186.605,100.414,32.008,16.000,0.000,24.000,28.000,128.000
enc-2nd-layernorm,88.124,260.478,16.000,32.000,0.000,16.016,0.000,128.000
enc-MLP-GEMM-1,1701.391,3470.118,32.000,48.016,64.000,32.000,0.000,176.000
enc-MLP-gelu,93.683,201.279,48.016,48.000,0.000,32.000,0.000,208.000
enc-MLP-GEMM-2,2237.830,2815.804,48.000,32.008,64.000,16.000,0.000,144.000
enc-post-MLP-dropout,187.290,100.682,32.008,16.000,0.000,24.000,28.000,128.000
final-layernorm,90.128,945.655,16.000,16.000,0.000,16.016,0.000,64.000
gpt-post-process,6991.111,10121.454,16.000,0.000,200.000,200.025,99.975,0.000
