op_name,forward-compute,backward-compute,input_size,output_size,weights,activations,fwd_reserved,bwd_reserved
encoder-embedding,1093.640,2178.246,0.008,20.000,270.000,30.018,19.982,330.000
enc-1st-layernorm,115.727,332.443,20.000,40.000,0.000,20.016,0.000,160.000
enc-attention-qkv,2000.823,4142.026,40.000,50.000,75.000,30.000,30.000,270.000
enc-attention-score,523.336,739.367,50.000,190.000,0.000,160.000,160.000,350.000
enc-attention-softmax,384.525,525.931,190.000,190.000,0.000,160.000,0.000,480.000
enc-attention-dropout,602.189,519.072,190.000,190.000,0.000,240.000,0.000,640.000
enc-attention-context,411.000,845.213,190.000,30.000,0.000,10.000,10.000,190.000
enc-attention-dense,1338.608,1065.290,30.000,40.010,25.000,20.000,0.000,0.000
enc-post-attention-dropout,232.699,123.862,40.010,20.000,0.000,30.000,20.000,160.000
enc-2nd-layernorm,117.656,331.529,20.000,40.000,0.000,20.016,0.000,160.000
enc-MLP-GEMM-1,2526.233,5035.329,40.000,60.020,100.000,40.000,0.000,240.000
enc-MLP-gelu,117.018,229.416,60.020,60.000,0.000,40.000,0.000,240.000
enc-MLP-GEMM-2,3197.352,4412.261,60.000,40.010,100.000,20.000,0.000,200.000
enc-post-MLP-dropout,232.750,123.581,40.010,20.000,0.000,30.000,20.000,160.000
final-layernorm,119.566,944.875,20.000,20.000,0.000,20.016,0.000,80.000
gpt-post-process,8228.441,11943.256,20.000,0.000,250.000,200.025,99.975,250.000
