op_name,forward-compute,backward-compute,input_size,output_size,weights,activations,fwd_reserved,bwd_reserved
encoder-embedding,970.799,1511.866,0.008,20.000,145.000,30.018,0.000,226.000
enc-1st-layernorm,115.286,331.382,20.000,40.000,0.000,20.016,0.000,160.000
enc-attention-qkv,2761.205,1893.755,25.000,80.000,37.500,60.000,60.000,264.000
enc-attention-score,248.675,330.172,50.000,120.000,0.000,80.000,80.000,180.000
enc-attention-softmax,2143.947,3489.896,120.000,120.000,0.000,240.000,160.000,560.000
enc-attention-dropout,301.986,261.014,120.000,120.000,0.000,120.000,0.000,320.000
enc-attention-context,173.868,378.783,120.000,40.000,0.000,5.000,0.000,90.000
enc-attention-dense,917.790,557.998,40.000,55.010,12.500,20.000,0.000,94.000
enc-post-attention-dropout,230.178,132.446,55.010,35.000,0.000,30.000,20.000,160.000
enc-2nd-layernorm,115.948,333.451,35.000,55.000,0.000,20.016,0.000,160.000
enc-MLP-GEMM-1,1250.077,2927.885,55.000,55.010,50.000,20.000,0.000,130.000
enc-MLP-gelu,60.215,138.249,55.010,55.000,0.000,20.000,0.000,160.000
enc-MLP-GEMM-2,1798.589,2154.355,55.000,55.010,50.000,20.000,0.000,130.000
enc-post-MLP-dropout,232.224,123.523,55.010,35.000,0.000,30.000,20.000,160.000
final-layernorm,116.322,971.634,35.000,35.000,0.000,20.016,0.000,80.000
gpt-post-process,4335.880,6231.953,35.000,15.000,125.000,100.025,49.975,126.000
