diff --git a/megatron/arguments.py b/megatron/arguments.py
index b8c230f..d9359a9 100644
--- a/megatron/arguments.py
+++ b/megatron/arguments.py
@@ -19,6 +19,7 @@ import argparse
 import os
 
 import torch
+import json 
 
 def parse_args(extra_args_provider=None, defaults={},
                ignore_unknown_args=False):
@@ -52,6 +53,27 @@ def parse_args(extra_args_provider=None, defaults={},
     else:
         args = parser.parse_args()
 
+    # Modified for Aceso evaluation
+    if args.config_file is not None:
+
+        with open(args.config_file, "r") as f:
+            config_dict = json.load(f)
+
+        args.num_layers = config_dict["num_layers"]
+        args.hidden_size = config_dict["hidden_size"]
+        args.num_attention_heads = config_dict["num_attention_heads"]
+
+        args.micro_batch_size = config_dict["micro_batch_size"]//config_dict["data_parallel_size_of_each_op"][0][0]
+        args.global_batch_size = config_dict["global_batch_size"]
+        args.seq_length = config_dict["seq_length"]
+        args.max_position_embeddings = config_dict["max_position_embeddings"]
+
+        args.tensor_model_parallel_size = config_dict["model_parallel_size_of_each_op"][0][0]
+        args.pipeline_model_parallel_size = config_dict["num_stages"]
+
+        args.checkpoint_activations = config_dict["checkpoint_activations"][0]
+        args.log_name = args.config_file.split("/")[-1].split(".json")[0]
+
     # Distributed args.
     args.rank = int(os.getenv('RANK', '0'))
     args.world_size = int(os.getenv("WORLD_SIZE", '1'))
@@ -295,7 +317,7 @@ def _add_network_size_args(parser):
     group.add_argument('--bert-no-binary-head', action='store_false',
                        help='Disable BERT binary head.',
                        dest='bert_binary_head')
-
+    group.add_argument('--config-file', type=str, default=None, help='config file generated by grid search.')
     return parser
 
 
@@ -328,6 +350,10 @@ def _add_logging_args(parser):
                        action='store_true',
                        help='If set, write validation perplexity to '
                        'tensorboard.')
+    
+    group.add_argument('--log-path', type=str, default="./", help='')  
+    group.add_argument('--log-name', type=str, default="tmp_log", help='')  
+
 
     return parser
 
diff --git a/megatron/global_vars.py b/megatron/global_vars.py
index c486f0d..d80c1c5 100644
--- a/megatron/global_vars.py
+++ b/megatron/global_vars.py
@@ -249,14 +249,19 @@ class Timers:
     def log(self, names, normalizer=1.0, reset=True):
         """Log a group of timers."""
         assert normalizer > 0.0
+        time_to_csv = [[],[]]
         string = 'time (ms)'
         for name in names:
             elapsed_time = self.timers[name].elapsed(
                 reset=reset) * 1000.0 / normalizer
             string += ' | {}: {:.2f}'.format(name, elapsed_time)
+            time_to_csv[0].append(name)
+            time_to_csv[1].append(f"{elapsed_time:.2f}")
         if torch.distributed.is_initialized():
             if torch.distributed.get_rank() == (
                     torch.distributed.get_world_size() - 1):
                 print(string, flush=True)
         else:
             print(string, flush=True)
+
+        return time_to_csv
\ No newline at end of file
diff --git a/megatron/training.py b/megatron/training.py
index 72a430e..e9082da 100644
--- a/megatron/training.py
+++ b/megatron/training.py
@@ -52,7 +52,7 @@ from megatron.schedules import forward_backward_pipelining_without_interleaving
 from megatron.schedules import forward_backward_pipelining_with_interleaving
 from megatron.utils import report_memory
 
-
+import csv
 
 def print_datetime(string):
     """Note that this call will sync across all ranks."""
@@ -574,7 +574,14 @@ def training_log(loss_dict, total_loss_dict, learning_rate, iteration,
             # Report memory after optimizer state has been initialized.
             report_memory('(after {} iterations)'.format(iteration))
             report_memory_flag = False
-        timers.log(timers_to_log, normalizer=args.log_interval)
+        _time_to_csv = timers.log(timers_to_log, normalizer=args.log_interval)
+
+        if iteration == (args.train_iters - 1):
+            time_to_csv = [["global_batch_size", "time"] + _time_to_csv[0], [batch_size, f"{elapsed_time_per_iteration * 1000.0:.2f}"] + _time_to_csv[1]]
+            with open(f"{args.log_path}csv/{args.log_name}_rank{torch.distributed.get_rank()}.csv", mode="w", newline="") as file:
+                writer = csv.writer(file)
+                for row in time_to_csv:
+                    writer.writerow(row)
 
     return report_memory_flag
 
@@ -823,9 +830,14 @@ def build_train_valid_test_data_iterators(
         test_dataloader = build_pretraining_data_loader(test_ds, 0)
 
         # Flags to know if we need to do training/validation/testing.
-        do_train = train_dataloader is not None and args.train_iters > 0
-        do_valid = valid_dataloader is not None and args.eval_iters > 0
-        do_test = test_dataloader is not None and args.eval_iters > 0
+        # do_train = train_dataloader is not None and args.train_iters > 0
+        # do_valid = valid_dataloader is not None and args.eval_iters > 0
+        # do_test = test_dataloader is not None and args.eval_iters > 0
+
+        do_train = args.train_iters > 0
+        do_valid = args.eval_iters > 0
+        do_test = args.eval_iters > 0      
+
         # Need to broadcast num_tokens and num_type_tokens.
         flags = torch.cuda.LongTensor(
             [int(do_train), int(do_valid), int(do_test)])
diff --git a/pretrain_gpt.py b/pretrain_gpt.py
index d8f9317..8d52c74 100644
--- a/pretrain_gpt.py
+++ b/pretrain_gpt.py
@@ -16,6 +16,7 @@
 """Pretrain GPT"""
 
 import torch
+torch.manual_seed(1234)
 from functools import partial
 from megatron import get_args
 from megatron import print_rank_0
@@ -28,6 +29,10 @@ from megatron.training import pretrain
 from megatron.utils import get_ltor_masks_and_position_ids
 from megatron.utils import average_losses_across_data_parallel_group
 
+import os
+SYNTHETIC_DATA = os.environ.get("SYNTHETIC_DATA", '1') == '1'
+
+
 def model_provider(pre_process=True, post_process=True):
     """Build the model."""
 
@@ -40,35 +45,40 @@ def model_provider(pre_process=True, post_process=True):
     )
     return model
 
-
 def get_batch(data_iterator):
     """Generate a batch"""
     args = get_args()
-    tokenizer = get_tokenizer()
 
-    # Items and their type.
-    keys = ['text']
-    datatype = torch.int64
-
-    # Broadcast data.
-    if data_iterator is not None:
-        data = next(data_iterator)
+    if SYNTHETIC_DATA:
+        vocab_size = 50257
+        tokens = torch.ones((args.micro_batch_size, args.seq_length), requires_grad=False, device=torch.cuda.current_device()).long() * vocab_size
+        # labels = torch.rand((args.micro_batch_size, args.seq_length), requires_grad=False, device=torch.cuda.current_device()).long() * vocab_size
+        labels = torch.ones((args.micro_batch_size, args.seq_length), requires_grad=False, device=torch.cuda.current_device()).long() * vocab_size 
+        loss_mask =  (torch.rand((args.micro_batch_size, args.seq_length), requires_grad=False, device=torch.cuda.current_device()) < 0.5).float()
+        attention_mask = (torch.rand((args.micro_batch_size, 1, args.seq_length, args.seq_length), requires_grad=False, device=torch.cuda.current_device()) < 0.5)
+        position_ids = torch.rand((args.micro_batch_size, args.seq_length), requires_grad=False, device=torch.cuda.current_device()).long() * args.seq_length
     else:
-        data = None
-    data_b = mpu.broadcast_data(keys, data, datatype)
-
-    # Unpack.
-    tokens_ = data_b['text'].long()
-    labels = tokens_[:, 1:].contiguous()
-    tokens = tokens_[:, :-1].contiguous()
-
-    # Get the masks and postition ids.
-    attention_mask, loss_mask, position_ids = get_ltor_masks_and_position_ids(
-        tokens,
-        tokenizer.eod,
-        args.reset_position_ids,
-        args.reset_attention_mask,
-        args.eod_mask_loss)
+        tokenizer = get_tokenizer()
+        # Items and their type.
+        keys = ['text']
+        datatype = torch.int64
+        # Broadcast data.
+        if data_iterator is not None:
+            data = next(data_iterator)
+        else:
+            data = None
+        data_b = mpu.broadcast_data(keys, data, datatype)
+        # Unpack.
+        tokens_ = data_b['text'].long()
+        labels = tokens_[:, 1:].contiguous()
+        tokens = tokens_[:, :-1].contiguous()
+        # Get the masks and postition ids.
+        attention_mask, loss_mask, position_ids = get_ltor_masks_and_position_ids(
+            tokens,
+            tokenizer.eod,
+            args.reset_position_ids,
+            args.reset_attention_mask,
+            args.eod_mask_loss)
 
     return tokens, labels, loss_mask, attention_mask, position_ids
 
@@ -102,21 +112,24 @@ def forward_step(data_iterator, model):
 
 def train_valid_test_datasets_provider(train_val_test_num_samples):
     """Build train, valid, and test datasets."""
-    args = get_args()
-
-    print_rank_0('> building train, validation, and test datasets '
-                 'for GPT ...')
-    train_ds, valid_ds, test_ds = build_train_valid_test_datasets(
-        data_prefix=args.data_path,
-        data_impl=args.data_impl,
-        splits_string=args.split,
-        train_valid_test_num_samples=train_val_test_num_samples,
-        seq_length=args.seq_length,
-        seed=args.seed,
-        skip_warmup=(not args.mmap_warmup))
-    print_rank_0("> finished creating GPT datasets ...")
-
-    return train_ds, valid_ds, test_ds
+    if SYNTHETIC_DATA:
+        return None, None, None
+    else:    
+        args = get_args()
+
+        print_rank_0('> building train, validation, and test datasets '
+                    'for GPT ...')
+        train_ds, valid_ds, test_ds = build_train_valid_test_datasets(
+            data_prefix=args.data_path,
+            data_impl=args.data_impl,
+            splits_string=args.split,
+            train_valid_test_num_samples=train_val_test_num_samples,
+            seq_length=args.seq_length,
+            seed=args.seed,
+            skip_warmup=(not args.mmap_warmup))
+        print_rank_0("> finished creating GPT datasets ...")
+
+        return train_ds, valid_ds, test_ds
 
 
 if __name__ == "__main__":
